{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06d7f765-2ed1-4401-93f5-6c161f5b45e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5b450e-24f1-4a2d-8a4e-78cc999c0a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\ACM AI Project\\Face_Recognition\n",
      "G:\\ACM AI Project\\Face_Recognition\\data\\valid\n"
     ]
    }
   ],
   "source": [
    "# get path to data folder\n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "data_dir = pathlib.Path.cwd()\n",
    "data_train_dir = data_dir.joinpath('data','train')\n",
    "data_val_dir = data_dir.joinpath('data', 'valid')\n",
    "print(data_val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f3fce3-470f-425d-8db1-29fd943e2c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# get all emotion class names\n",
    "emotions = os.listdir(data_train_dir)\n",
    "print(emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dbc3ec-b257-4cf2-b075-18f25e447dcf",
   "metadata": {},
   "source": [
    "## Find the minimum number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b6d8b1-b871-41a6-8ac8-962e4ba7cb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6566, 3231, 4859, 28592, 29384, 12223, 8113]\n"
     ]
    }
   ],
   "source": [
    "num_emotion_file_arr = []\n",
    "\n",
    "# get number of files in each emotion folders\n",
    "for emotion in emotions:\n",
    "    emotion_files = list(data_train_dir.glob(emotion + '/*'))\n",
    "    num_emotion_file_arr.append(len(emotion_files))\n",
    "\n",
    "print(num_emotion_file_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a63b0002-1039-4ec0-89f7-f6af0dddd4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1017, 656, 659, 5475, 5839, 2236, 1474]\n"
     ]
    }
   ],
   "source": [
    "num_val_file_arr = []\n",
    "\n",
    "# get number of files in each emotion folders\n",
    "for emotion in emotions:\n",
    "    emotion_files = list(data_val_dir.glob(emotion + '/*'))\n",
    "    num_val_file_arr.append(len(emotion_files))\n",
    "\n",
    "print(num_val_file_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b9c0bc-9650-4077-aca9-ca2672903971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3231\n"
     ]
    }
   ],
   "source": [
    "num_files = min(num_emotion_file_arr)\n",
    "print(num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "996beef0-5353-4923-b786-015f3f2e064d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656\n"
     ]
    }
   ],
   "source": [
    "num_val_files = min(num_val_file_arr)\n",
    "print(num_val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e99e244-cb69-4592-a36a-e27393c76bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3231\n",
      "G:\\ACM AI Project\\Face_Recognition\\data\\train\\surprise\\*\n"
     ]
    }
   ],
   "source": [
    "random_files = np.random.choice(emotion_files, num_files)\n",
    "print(len(random_files))\n",
    "print(str(data_train_dir.joinpath(emotion,'*')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2da58-8050-4430-bb0a-0609b57601c9",
   "metadata": {},
   "source": [
    "## Preprocess train data and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31f516-3ac3-476c-9a9a-5788b41a69c8",
   "metadata": {},
   "source": [
    "### Get list of image file path and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b2ba9e0-0fce-4c10-9720-562eea775b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files=[]\n",
    "labels=[]\n",
    "\n",
    "for emotion in emotions:\n",
    "    label = emotions.index(emotion)\n",
    "    \n",
    "    emotion_files = glob.glob(str(data_train_dir.joinpath(emotion,'*')))\n",
    "    constrained_files = np.random.choice(emotion_files, num_files)\n",
    "    image_files.append(constrained_files)  \n",
    "    labels.append([label]*num_files)\n",
    "\n",
    "image_files = np.asarray(image_files)\n",
    "image_files = list(image_files.flatten())\n",
    "labels = np.asarray(labels)\n",
    "labels = list(labels.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e017f4-a944-407a-8ace-7379e1ebb214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4592\n"
     ]
    }
   ],
   "source": [
    "val_image_files=[]\n",
    "val_labels=[]\n",
    "\n",
    "for emotion in emotions:\n",
    "    label = emotions.index(emotion)\n",
    "    \n",
    "    emotion_files = glob.glob(str(data_val_dir.joinpath(emotion,'*')))\n",
    "    constrained_files = np.random.choice(emotion_files, num_val_files)\n",
    "    val_image_files.append(constrained_files)  \n",
    "    val_labels.append([label]*num_val_files)\n",
    "\n",
    "val_image_files = np.asarray(val_image_files)\n",
    "val_image_files = list(val_image_files.flatten())\n",
    "val_labels = np.asarray(val_labels)\n",
    "val_labels = list(val_labels.flatten())\n",
    "print(len(val_image_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9808ee2-da7e-41aa-b5b4-8c0b57bd5e38",
   "metadata": {},
   "source": [
    "### Get all images and store in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "584508cb-6928-4abd-a9d8-cb6ef608e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 48\n",
    "img_width = 48\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d004c95-6492-4222-9cc6-e39e7c34064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(filename, label):\n",
    "    image_string = tf.io.read_file(filename)\n",
    "\n",
    "    #Don't use tf.image.decode_image, or the output shape will be undefined\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "\n",
    "    #This will convert to float values in [0, 1]\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "    image = tf.image.resize(image, [48, 48])\n",
    "    \n",
    "    return image, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2680b229-97a7-4db3-8bbc-7e8b03981aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocess(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n",
    "    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "\n",
    "    #Make sure the image is still in [0, 1]\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92102802-3f80-4082-86f3-d423a91be875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_for_performance(ds):\n",
    "    ds = ds.cache()\n",
    "    ds = ds.shuffle(len(image_files))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eca018f3-48db-49d0-b097-4e6de00c253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((image_files, labels))\n",
    "\n",
    "train_ds = train_ds.map(parse_function, num_parallel_calls=4)\n",
    "train_ds = train_ds.map(train_preprocess, num_parallel_calls=4)\n",
    "train_ds = configure_for_performance(train_ds)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_image_files, val_labels))\n",
    "val_ds = val_ds.map(parse_function, num_parallel_calls=4)\n",
    "val_ds = val_ds.map(train_preprocess, num_parallel_calls=4)\n",
    "val_ds = configure_for_performance(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ddd49c9-e98a-404e-b9d0-a3f4ebc27c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (32, 48, 48, 3)\n",
      "Label:  (32,)\n"
     ]
    }
   ],
   "source": [
    "for image, label in train_ds.take(1):\n",
    "    print(\"Image shape: \", image.numpy().shape)\n",
    "    print(\"Label: \", label.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "112f79a6-e582-4ad9-b1c5-eb14fcc718f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(train_ds))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "    label = label_batch[i]\n",
    "    plt.title(emotions[label])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a0cab8-b017-4223-99b6-e093decb3902",
   "metadata": {},
   "source": [
    "### Test simple CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d6a3b13-0ccc-4e28-8bab-865d91cf1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41e5fa99-d405-485e-a31b-677b81f8c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a1b5484-a571-4549-9f6d-3d4c8d131717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "707/707 [==============================] - 31s 43ms/step - loss: 1.8599 - accuracy: 0.2297 - val_loss: 1.8060 - val_accuracy: 0.2646\n",
      "Epoch 2/10\n",
      "707/707 [==============================] - 23s 32ms/step - loss: 1.6757 - accuracy: 0.3433 - val_loss: 1.8002 - val_accuracy: 0.3084\n",
      "Epoch 3/10\n",
      "707/707 [==============================] - 23s 32ms/step - loss: 1.5427 - accuracy: 0.4008 - val_loss: 1.7407 - val_accuracy: 0.3249\n",
      "Epoch 4/10\n",
      "707/707 [==============================] - 23s 33ms/step - loss: 1.4308 - accuracy: 0.4483 - val_loss: 1.7104 - val_accuracy: 0.3434\n",
      "Epoch 5/10\n",
      "707/707 [==============================] - 23s 33ms/step - loss: 1.3213 - accuracy: 0.4978 - val_loss: 1.7093 - val_accuracy: 0.3458\n",
      "Epoch 6/10\n",
      "707/707 [==============================] - 23s 32ms/step - loss: 1.2113 - accuracy: 0.5456 - val_loss: 1.8270 - val_accuracy: 0.3341\n",
      "Epoch 7/10\n",
      "707/707 [==============================] - 24s 35ms/step - loss: 1.0768 - accuracy: 0.6025 - val_loss: 1.8937 - val_accuracy: 0.3338\n",
      "Epoch 8/10\n",
      "707/707 [==============================] - 24s 34ms/step - loss: 0.9589 - accuracy: 0.6484 - val_loss: 1.9677 - val_accuracy: 0.3199\n",
      "Epoch 9/10\n",
      "707/707 [==============================] - 25s 35ms/step - loss: 0.8293 - accuracy: 0.7019 - val_loss: 2.1891 - val_accuracy: 0.3277\n",
      "Epoch 10/10\n",
      "707/707 [==============================] - 24s 33ms/step - loss: 0.6844 - accuracy: 0.7602 - val_loss: 2.3728 - val_accuracy: 0.3075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22f0af71970>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0297d5-0532-4c83-8b5d-31cbe1e7223a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
